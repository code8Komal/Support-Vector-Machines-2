{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d44c566-0267-4735-8d4d-9787740a3e9a",
   "metadata": {},
   "source": [
    "## Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d035a0-751d-4566-b83c-2b03e3c11103",
   "metadata": {},
   "source": [
    "In machine learning algorithms, polynomial functions and kernel functions are related concepts, especially in the context of Support Vector Machines (SVMs) and other kernel-based methods. Let's explore their relationship:\n",
    "\n",
    "**1. Polynomial Functions**:\n",
    "Polynomial functions are mathematical functions defined by expressions involving powers of a variable. A polynomial of degree \\( d \\) is given by the equation:\n",
    "\n",
    "\\[ f(x) = a_0 + a_1 x + a_2 x^2 + \\ldots + a_d x^d \\]\n",
    "\n",
    "where \\( x \\) is the variable, \\( a_0, a_1, \\ldots, a_d \\) are coefficients, and \\( d \\) is the degree of the polynomial.\n",
    "\n",
    "In the context of machine learning, polynomial functions are often used to create non-linear decision boundaries or feature transformations. For example, in polynomial regression, we fit a polynomial function to the data to model non-linear relationships between features and target variables.\n",
    "\n",
    "**2. Kernel Functions**:\n",
    "Kernel functions are similarity measures that compute the inner product (dot product) between two feature vectors in a higher-dimensional feature space. Kernel functions allow us to implicitly map the data into a higher-dimensional space without explicitly computing the transformations.\n",
    "\n",
    "In SVMs, kernel functions play a crucial role in capturing non-linear relationships between features by transforming the input data into a higher-dimensional space where the classes may become linearly separable. Commonly used kernel functions include the polynomial kernel, Gaussian radial basis function (RBF) kernel, and sigmoid kernel.\n",
    "\n",
    "**Relationship**:\n",
    "\n",
    "The relationship between polynomial functions and kernel functions lies in the fact that polynomial functions can be used as kernel functions in SVMs to induce non-linear decision boundaries. Specifically, the polynomial kernel is defined as:\n",
    "\n",
    "\\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)^d \\]\n",
    "\n",
    "where \\( \\mathbf{x}_i \\) and \\( \\mathbf{x}_j \\) are feature vectors, \\( \\gamma \\) is a scaling factor, \\( r \\) is an offset, and \\( d \\) is the degree of the polynomial.\n",
    "\n",
    "This kernel function computes the inner product of the transformed feature vectors in a higher-dimensional space, where the features are polynomial combinations of the original features up to degree \\( d \\). By using polynomial kernel functions, SVMs can capture non-linear relationships between features and achieve non-linear classification boundaries.\n",
    "\n",
    "In summary, polynomial functions can be used as kernel functions in SVMs to induce non-linear decision boundaries, allowing SVMs to effectively handle non-linearly separable data and capture complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e1433-a667-44cd-b15d-b952d53883df",
   "metadata": {},
   "source": [
    "## Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430d361d-fc71-476c-afac-7c552cd0105d",
   "metadata": {},
   "source": [
    "## To implement an SVM with a polynomial kernel in Python using scikit-learn, you can use the SVC (Support Vector Classifier) class with the kernel parameter set to 'poly'. Additionally, you can specify the degree of the polynomial kernel using the degree parameter. Here's how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c39c6c-e9fb-44d4-8621-0a7293ac85a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM with polynomial kernel: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3)  # Degree of the polynomial kernel (default is 3)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of SVM with polynomial kernel:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e4620-556b-4098-9abb-050aa72fa034",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "We load the Iris dataset using datasets.load_iris().\n",
    "We split the dataset into a training set (X_train, y_train) and a testing set (X_test, y_test) using train_test_split().\n",
    "We create an SVM classifier with a polynomial kernel using SVC(kernel='poly', degree=3). Here, degree=3 specifies the degree of the polynomial kernel (default is 3).\n",
    "We train the SVM classifier on the training set using the fit() method.\n",
    "We predict the labels for the testing set using the predict() method.\n",
    "We compute the accuracy of the model using accuracy_score().\n",
    "You can adjust the degree parameter to change the degree of the polynomial kernel and experiment with different values to see how it affects the performance of the SVM classifier. Additionally, you can tune other hyperparameters such as C (regularization parameter) and gamma (kernel coefficient) to further optimize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f078320-f3cc-461f-bbd1-bae328ae2590",
   "metadata": {},
   "source": [
    "## Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4a3816-ad90-4f41-8d46-856d0ceff89a",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the parameter epsilon (Îµ) defines the margin of tolerance around the regression line within which no penalty is associated in the loss function. This margin of tolerance determines the tube within which data points are considered to be correctly predicted and do not contribute to the loss function.\n",
    "\n",
    "The relationship between the value of epsilon and the number of support vectors in SVR is as follows:\n",
    "\n",
    "1. **Small Epsilon**: When the value of epsilon is small, the margin of tolerance around the regression line is narrow. This means that the SVR model aims to fit the training data more tightly, allowing fewer data points to lie within the margin of tolerance. As a result, the model is more sensitive to individual data points, and the number of support vectors tends to increase.\n",
    "\n",
    "2. **Large Epsilon**: Conversely, when the value of epsilon is large, the margin of tolerance around the regression line is wider. This allows more data points to lie within the margin of tolerance without incurring a penalty in the loss function. Consequently, the SVR model prioritizes capturing the general trend of the data rather than fitting individual data points closely. As a result, the number of support vectors tends to decrease.\n",
    "\n",
    "In summary, increasing the value of epsilon in SVR tends to reduce the number of support vectors, as it allows for a wider margin of tolerance and a smoother regression line. Conversely, decreasing the value of epsilon leads to a tighter fit to the training data and an increase in the number of support vectors. The choice of epsilon should be carefully considered based on the specific characteristics of the dataset and the desired trade-off between model complexity and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7df02c-9c60-4d64-89b4-3f3854b0987f",
   "metadata": {},
   "source": [
    "## Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d03fa7-8078-4043-93ad-0284d0d083c5",
   "metadata": {},
   "source": [
    "Support Vector Regression (SVR) is a powerful machine learning algorithm for regression tasks that uses support vector machines (SVMs) to find the best-fitting line or hyperplane. The performance of SVR can be significantly influenced by several key parameters:\n",
    "\n",
    "1. **Kernel Function**: The choice of kernel function determines the type of non-linear transformation applied to the feature space. Common kernel functions include:\n",
    "   - Linear: Suitable for linearly separable or linearly related data.\n",
    "   - Polynomial: Introduces non-linearity through polynomial transformations.\n",
    "   - Gaussian Radial Basis Function (RBF): Suitable for capturing complex non-linear relationships.\n",
    "\n",
    "   **Effect**: The choice of kernel function affects the flexibility of the SVR model and its ability to capture complex patterns in the data. For example, if the relationship between features and target variable is non-linear, using a polynomial or RBF kernel may yield better performance than a linear kernel.\n",
    "\n",
    "2. **C Parameter**: The C parameter controls the trade-off between minimizing the training error and minimizing the model complexity (i.e., the number of support vectors). A smaller value of C allows for a larger margin and more misclassifications, while a larger value of C penalizes misclassifications more heavily, leading to a tighter fit to the training data.\n",
    "\n",
    "   **Effect**: Increasing the value of C makes the SVR model more sensitive to individual data points, potentially resulting in overfitting. Conversely, decreasing the value of C allows the model to generalize better to unseen data, but may lead to underfitting.\n",
    "\n",
    "3. **Epsilon Parameter**: The epsilon parameter (Îµ) defines the margin of tolerance around the regression line within which no penalty is associated in the loss function. It determines the width of the tube within which data points are considered to be correctly predicted.\n",
    "\n",
    "   **Effect**: A smaller value of epsilon results in a narrower tube, making the SVR model more sensitive to deviations from the regression line. Conversely, a larger value of epsilon allows for more flexibility in fitting the data and reduces the risk of overfitting, but may sacrifice precision.\n",
    "\n",
    "4. **Gamma Parameter**: The gamma parameter (Î³) defines the kernel coefficient for kernel functions such as the RBF kernel. It controls the influence of individual training samples on the decision boundary. A low value of gamma indicates a large similarity radius, while a high value of gamma results in a smaller similarity radius.\n",
    "\n",
    "   **Effect**: Increasing the value of gamma makes the SVR model more sensitive to local variations in the data, potentially leading to overfitting. On the other hand, decreasing the value of gamma can improve generalization by considering a wider range of data points.\n",
    "\n",
    "**Example Scenarios**:\n",
    "- Increase C: When the training data contains noise or outliers, increasing the value of C can help the SVR model fit the data more tightly and reduce the influence of outliers.\n",
    "- Decrease C: When the training data is noisy or when the goal is to prevent overfitting, decreasing the value of C can improve the generalization ability of the SVR model.\n",
    "- Increase Epsilon: When the training data exhibits significant variability or uncertainty, increasing the value of epsilon can allow for a wider margin of tolerance and improve the robustness of the SVR model to noise.\n",
    "- Adjust Gamma: When dealing with non-linear relationships in the data, experimenting with different values of gamma can help find the optimal balance between model complexity and generalization.\n",
    "\n",
    "Overall, the choice of kernel function, C parameter, epsilon parameter, and gamma parameter in SVR depends on the specific characteristics of the dataset, the complexity of the underlying relationship, and the trade-off between model flexibility and generalization performance. Experimentation and cross-validation are often necessary to determine the optimal parameter values for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf3539-0efd-47d2-bb5c-4657166478bf",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataseg\n",
    "L Split the dataset into training and testing setZ\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "L hse the trained classifier to predict the labels of the testing datW\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "L Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccf01215-6f33-4bc2-8aa9-6400fb1df08e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the dataset (replace 'dataset.csv' with the actual file path)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Separate features and target variable\u001b[39;00m\n\u001b[1;32m     15\u001b[0m X \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.csv'"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the dataset (replace 'dataset.csv' with the actual file path)\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataset.drop('target', axis=1)\n",
    "y = dataset['target']\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data using standardization (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate the performance using classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Tune the hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': [0.1, 0.01, 0.001]}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_svc_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svc_classifier.fit(X_scaled, y)\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(best_svc_classifier, 'tuned_svc_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13b7da-c7c4-4b8d-9b3d-b01839cc0fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
